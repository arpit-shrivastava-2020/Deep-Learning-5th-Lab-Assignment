{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcuJW2xkzpC8"
      },
      "source": [
        "#GitHub Link:https://github.com/arpit-shrivastava-2020/Deep-Learning-5th-Lab-Assignment.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgreUS1Fxaov",
        "outputId": "37b79cf6-8202-4615-ecb8-c878915089c7"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def get_initial_loss(vocab_size, seq_length):\n",
        "    return -np.log(1.0/vocab_size)*seq_length\n",
        "\n",
        "def smooth(loss, cur_loss):\n",
        "    return loss * 0.999 + cur_loss * 0.001\n",
        "\n",
        "def print_sample(sample_ix, ix_to_char):\n",
        "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "    txt = txt[0].upper() + txt[1:]  # capitalize first character\n",
        "    print('%s' % (txt, ), end='')\n",
        "\n",
        "\n",
        "def initialize_parameters(n_a, n_x, n_y):\n",
        "    Wax = np.random.randn(n_a, n_x)*0.01  # input to hidden\n",
        "    Waa = np.random.randn(n_a, n_a)*0.01  # hidden to hidden\n",
        "    Wya = np.random.randn(n_y, n_a)*0.01  # hidden to output\n",
        "    ba = np.zeros((n_a, 1))  # hidden bias\n",
        "    by = np.zeros((n_y, 1))  # output bias\n",
        "\n",
        "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def update_parameters(parameters, gradients, lr):\n",
        "\n",
        "    parameters['Wax'] += -lr * gradients['dWax']\n",
        "    parameters['Waa'] += -lr * gradients['dWaa']\n",
        "    parameters['Wya'] += -lr * gradients['dWya']\n",
        "    parameters['ba'] += -lr * gradients['db']\n",
        "    parameters['by'] += -lr * gradients['dby']\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "def rnn_cell_forward(xt, a_prev, parameters):\n",
        "    \n",
        "    Wax = parameters[\"Wax\"]\n",
        "    Waa = parameters[\"Waa\"]\n",
        "    Wya = parameters[\"Wya\"]\n",
        "    ba = parameters[\"ba\"]\n",
        "    by = parameters[\"by\"]\n",
        "\n",
        "    a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)\n",
        "    yt_pred = softmax(np.dot(Wya, a_next) + by)\n",
        "\n",
        "    cache = (a_next, a_prev, xt, parameters)\n",
        "\n",
        "    return a_next, yt_pred, cache\n",
        "\n",
        "def rnn_forward(X, Y, a0, parameters, vocab_size=27):\n",
        "    x, a, y_hat = {}, {}, {}\n",
        "    a[-1] = np.copy(a0)\n",
        "    loss = 0\n",
        "    for t in range(len(X)):\n",
        "        x[t] = np.zeros((vocab_size, 1))\n",
        "        if (X[t] != None):\n",
        "            x[t][X[t]] = 1\n",
        "        a[t], y_hat[t], _ = rnn_cell_forward(x[t], a[t-1],parameters)\n",
        "        loss -= np.log(y_hat[t][Y[t], 0])\n",
        "    cache = (y_hat, a, x)\n",
        "    return loss, cache\n",
        "\n",
        "def rnn_cell_backward(dy, gradients, parameters, x, a, a_prev):\n",
        "    gradients['dWya'] += np.dot(dy, a.T)\n",
        "    gradients['dby'] += dy\n",
        "    da = np.dot(parameters['Wya'].T, dy) +         gradients['da_next']  # backprop into h\n",
        "    daraw = (1 - a * a) * da  # backprop through tanh nonlinearity\n",
        "    gradients['db'] += daraw\n",
        "    gradients['dWax'] += np.dot(daraw, x.T)\n",
        "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
        "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
        "    return gradients\n",
        "\n",
        "\n",
        "def rnn_backward(X, Y, parameters, cache):\n",
        "    gradients = {}\n",
        "    (y_hat, a, x) = cache\n",
        "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
        "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
        "    gradients['db'], gradients['dby'] = np.zeros_like(ba), np.zeros_like(by)\n",
        "    gradients['da_next'] = np.zeros_like(a[0])\n",
        "\n",
        "    for t in reversed(range(len(X))):\n",
        "        dy = np.copy(y_hat[t])\n",
        "        dy[Y[t]] -= 1\n",
        "        gradients = rnn_cell_backward(\n",
        "            dy, gradients, parameters, x[t], a[t], a[t-1])\n",
        "    \n",
        "    return gradients, a\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "\n",
        "def clip(gradients, maxValue):\n",
        "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
        "   \n",
        "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
        "        np.clip(gradient, -maxValue, maxValue, out=gradient)\n",
        "    \n",
        "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
        "    \n",
        "    return gradients\n",
        "\n",
        "\n",
        "def sample(parameters, char_to_ix, seed):\n",
        "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
        "    vocab_size = by.shape[0]\n",
        "    n_a = Waa.shape[1]    \n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    a_prev = np.zeros((n_a, 1))\n",
        "    indices = []\n",
        "    \n",
        "    idx = -1 \n",
        "    \n",
        "    counter = 0\n",
        "    newline_character = char_to_ix['\\n']\n",
        "    \n",
        "    while (idx != newline_character and counter != 50):\n",
        "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + ba)\n",
        "        z = np.dot(Wya, a) + by\n",
        "        y = softmax(z)\n",
        "            \n",
        "        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n",
        "\n",
        "        indices.append(idx)\n",
        "        \n",
        "        x = np.zeros((vocab_size, 1))\n",
        "        x[idx] = 1\n",
        "        \n",
        "        a_prev = a\n",
        "        \n",
        "        counter +=1\n",
        "        \n",
        "    if (counter == 50):\n",
        "        indices.append(char_to_ix['\\n'])\n",
        "    \n",
        "    return indices\n",
        "\n",
        "\n",
        "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
        "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
        "    \n",
        "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
        "    \n",
        "    gradients = clip(gradients, 5)\n",
        "    \n",
        "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "        \n",
        "    return loss, gradients, a[len(X)-1]\n",
        "\n",
        "\n",
        "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n",
        "    \n",
        "    n_x, n_y = vocab_size, vocab_size\n",
        "    \n",
        "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
        "    \n",
        "    loss = get_initial_loss(vocab_size, dino_names)\n",
        "    \n",
        "    with open(\"dinos.txt\") as f:\n",
        "        examples = f.readlines()\n",
        "    examples = [x.lower().strip() for x in examples]\n",
        "    \n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(examples)\n",
        "    \n",
        "    a_prev = np.zeros((n_a, 1))\n",
        "    \n",
        "    for j in range(num_iterations):\n",
        "        \n",
        "        index = j % len(examples)\n",
        "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
        "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
        "        \n",
        "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n",
        "        \n",
        "        loss = smooth(loss, curr_loss)\n",
        "\n",
        "        if j % 2000 == 0:\n",
        "            \n",
        "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
        "            \n",
        "            seed = 0\n",
        "            for name in range(dino_names):\n",
        "                \n",
        "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
        "                txt = ''.join(ix_to_char[ix] for ix in sampled_indices)\n",
        "                txt = txt[0].upper() + txt[1:]  # capitalize first character\n",
        "                print('%s' % (txt, ), end='')\n",
        "                \n",
        "                seed += 1  \n",
        "      \n",
        "            print('\\n')\n",
        "        \n",
        "    return parameters\n",
        "\n",
        "data = open('dinos.txt', 'r').read()\n",
        "data= data.lower()\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))\n",
        "\n",
        "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
        "\n",
        "model(data, ix_to_char, char_to_ix)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 19909 total characters and 27 unique characters in your data.\n",
            "Iteration: 0, Loss: 23.087336\n",
            "\n",
            "Co\n",
            "Eyhqgyshubjerbjgmis\n",
            "Qfguyhvbqtdkxb\n",
            "Okuxhjmozrjgycsxqkeisqywvxgijorg\n",
            "Jhywwqinubuavuapmdctozzzcyflteoebci\n",
            "Kqjbagdzduoomziuojypckurxgtloyzwiwqk\n",
            "Tmutuhunlr\n",
            "\n",
            "\n",
            "Iteration: 2000, Loss: 27.882500\n",
            "\n",
            "Amychangwsacrusycripshurashudncqrusayraalrptacsaus\n",
            "Gonqneleprus\n",
            "Anisaurus\n",
            "Anqoshorostisaurur\n",
            "Ranelocsaurus\n",
            "Ehlgwyrovtonackytabtonos\n",
            "Calceradenderzotibheosaurus\n",
            "\n",
            "\n",
            "Iteration: 4000, Loss: 25.878239\n",
            "\n",
            "Gomatosaurus\n",
            "Minvhytosaurus\n",
            "Angthosaurus\n",
            "Vourosaurus\n",
            "Akaceosaurus\n",
            "Ynainnphosaurus\n",
            "Rugoruous\n",
            "\n",
            "\n",
            "Iteration: 6000, Loss: 24.652644\n",
            "\n",
            "Lorspgnomeortoraszurateostos\n",
            "Lildinhosaune\n",
            "Sisnrovrosavosaurus\n",
            "Stetintynlerrorosaurus\n",
            "Deorostosauri\n",
            "Enoeosaurusqsauhekerossaurosaurus\n",
            "Ahusaurus\n",
            "\n",
            "\n",
            "Iteration: 8000, Loss: 24.146473\n",
            "\n",
            "Annspalnatoruitelodon\n",
            "Ustanopemus\n",
            "Asalnplosceromeichamnan\n",
            "Lrochrator\n",
            "Alephon\n",
            "Hutesaurus\n",
            "Rropinisaurus\n",
            "\n",
            "\n",
            "Iteration: 10000, Loss: 23.899045\n",
            "\n",
            "Skyaramasaurosaurus\n",
            "Ngycama\n",
            "Ntorus\n",
            "Opaovonatous\n",
            "Angolosaurus\n",
            "Yharopokechenteospcukiiaemothosaurus\n",
            "Ologacteitaus\n",
            "\n",
            "\n",
            "Iteration: 12000, Loss: 23.290918\n",
            "\n",
            "Xochosaurus\n",
            "Nousosaurus\n",
            "Osakeraatongymachuhuanodos\n",
            "Solaasrosaurusileevolilalusodol\n",
            "Nbpoltornosaurus\n",
            "Apisaurus\n",
            "Lengolangia\n",
            "\n",
            "\n",
            "Iteration: 14000, Loss: 23.343193\n",
            "\n",
            "Akirotalorratcormyaertoryosaurus\n",
            "Eraxator\n",
            "Erichorosaurus\n",
            "Kocaprosimaurosuerelentonorosaurus\n",
            "Rydialieroton\n",
            "Pechyllus\n",
            "Arotogon\n",
            "\n",
            "\n",
            "Iteration: 16000, Loss: 23.303303\n",
            "\n",
            "Eshamosaurus\n",
            "Huaasaurus\n",
            "Stroosaurus\n",
            "Ligustaorosaurus\n",
            "Ywnoos\n",
            "Aclirin\n",
            "Munocurhamdesaudus\n",
            "\n",
            "\n",
            "Iteration: 18000, Loss: 22.906256\n",
            "\n",
            "Anxyplosaurus\n",
            "Ticancous\n",
            "Zosaptithikosaurus\n",
            "Ginteosaurus\n",
            "Sllosecor\n",
            "Hopynlus\n",
            "Sroknysosaurotes\n",
            "\n",
            "\n",
            "Iteration: 20000, Loss: 22.974733\n",
            "\n",
            "Kerazsaurus\n",
            "Lelgynsaurus\n",
            "Moteloa\n",
            "Engthusaurus\n",
            "Kmzhmenaptorytisaurus\n",
            "Hugosaurus\n",
            "Inlrochptapochus\n",
            "\n",
            "\n",
            "Iteration: 22000, Loss: 22.880018\n",
            "\n",
            "Iageingxetes\n",
            "Laruaus\n",
            "Tencnchosaurus\n",
            "Ngugopteosaurus\n",
            "Alureptochedor\n",
            "Mhsimosaurus\n",
            "Pepterperoteoperosaurus\n",
            "\n",
            "\n",
            "Iteration: 24000, Loss: 22.790296\n",
            "\n",
            "Cenausaurus\n",
            "Chrosus\n",
            "Saurlus\n",
            "Telonus\n",
            "Esvenaton\n",
            "Iacenatosaurus\n",
            "Ripitosaurus\n",
            "\n",
            "\n",
            "Iteration: 26000, Loss: 22.817906\n",
            "\n",
            "Arangarodon\n",
            "Anptonyx\n",
            "Fulanthanzsis\n",
            "Kkoteratops\n",
            "Anxupilong\n",
            "Yraazandyonigopingratoptorwanorsaurus\n",
            "Rhinkochiloceratrus\n",
            "\n",
            "\n",
            "Iteration: 28000, Loss: 22.766441\n",
            "\n",
            "Minvacenitan\n",
            "Juktasalosaurus\n",
            "Binchosaurusxeiniuadatia\n",
            "Leoncheltan\n",
            "Elsachalterechultetankasphihacturyiiasthomallasaur\n",
            "Scritosauruswbefititanicemawiaimiwosaurus\n",
            "Eusalongnocanes\n",
            "\n",
            "\n",
            "Iteration: 30000, Loss: 22.715750\n",
            "\n",
            "Tiaswus\n",
            "Ngyposaurus\n",
            "Spucoong\n",
            "Dracausaustraphria\n",
            "Lyalosaurus\n",
            "Nciulecos\n",
            "Labrsaurus\n",
            "\n",
            "\n",
            "Iteration: 32000, Loss: 22.376895\n",
            "\n",
            "Lacavaratia\n",
            "Entavedan\n",
            "Onasan\n",
            "Eucopiprllosaurus\n",
            "Bileacesaurus\n",
            "Xuacntananasaurus\n",
            "Saurus\n",
            "\n",
            "\n",
            "Iteration: 34000, Loss: 22.475821\n",
            "\n",
            "Ighousaurus\n",
            "Osaurus\n",
            "Hanghumambrohlida\n",
            "Monskutonophalomyrus\n",
            "Yekicsanaptosaerailia\n",
            "Berodostycerasoueus\n",
            "Jidiaelosimagahskus\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Waa': array([[-0.13480882,  0.34505466,  0.18556581, ...,  0.47646669,\n",
              "         -0.12709853,  0.19799195],\n",
              "        [-0.32431566, -0.52469842, -0.02991147, ...,  0.28943028,\n",
              "          0.18537565,  0.44393654],\n",
              "        [ 0.53009569,  0.01772332, -0.20278475, ...,  0.0162098 ,\n",
              "          0.14523874,  0.15921288],\n",
              "        ...,\n",
              "        [-0.03441569, -0.11002757,  0.44711859, ..., -0.01999878,\n",
              "         -0.04662057,  0.63608135],\n",
              "        [-0.03249371, -0.06668051,  0.14008852, ...,  0.18737067,\n",
              "         -0.22422363, -0.15068433],\n",
              "        [ 0.07076852,  0.40185271, -0.22940305, ..., -0.39883206,\n",
              "         -0.00433171,  0.44679378]]),\n",
              " 'Wax': array([[ 2.93592659e-03,  6.12294251e-01,  1.54743617e-01, ...,\n",
              "         -2.44025855e-02, -6.61036913e-01, -6.17074464e-01],\n",
              "        [ 4.79402704e-03,  6.27523789e-01,  7.12554976e-01, ...,\n",
              "          5.34954730e-01,  1.12468936e+00, -8.06353444e-01],\n",
              "        [ 3.26520651e-03,  1.97227898e-01,  5.36714606e-01, ...,\n",
              "         -5.24763469e-01, -5.82123534e-02, -2.22060906e-01],\n",
              "        ...,\n",
              "        [-2.02165477e-02, -1.23944743e+00, -2.47089508e-01, ...,\n",
              "          1.74559250e-02,  2.43243486e-01, -2.12805726e-01],\n",
              "        [ 2.07475013e-02, -1.97683164e+00,  1.08751395e-01, ...,\n",
              "          1.12353495e+00, -1.95586051e+00,  1.01445949e+00],\n",
              "        [-1.70006072e-03,  1.13672010e-01, -4.99882420e-01, ...,\n",
              "          2.55003429e-01, -4.65640842e-01,  1.14265325e-01]]),\n",
              " 'Wya': array([[-0.19239584,  0.00785972, -0.09660497, ..., -0.0035574 ,\n",
              "         -0.00447647,  0.42436507],\n",
              "        [-0.29725255, -0.03840829,  0.21757871, ..., -0.05921989,\n",
              "          0.45507512,  0.01766143],\n",
              "        [-0.02550721, -0.07257092,  0.17526989, ...,  0.03549053,\n",
              "          0.0628787 , -0.03472832],\n",
              "        ...,\n",
              "        [ 0.04284638,  0.18041416, -0.34430149, ...,  0.18600694,\n",
              "         -0.36474871, -0.35933459],\n",
              "        [ 0.31177587, -0.17888144,  0.37503158, ..., -0.30764853,\n",
              "          0.47571563, -0.06655637],\n",
              "        [-0.01585427,  0.16675876, -0.01139449, ..., -0.09956817,\n",
              "         -0.21310833,  0.09988954]]),\n",
              " 'ba': array([[ 6.48835801e-04],\n",
              "        [-1.12889018e-01],\n",
              "        [-7.39638596e-01],\n",
              "        [-1.33583821e+00],\n",
              "        [ 7.84902245e-01],\n",
              "        [ 5.10785975e-02],\n",
              "        [ 3.07903879e-01],\n",
              "        [-9.86878073e-01],\n",
              "        [-1.02320189e+00],\n",
              "        [-4.22906731e-01],\n",
              "        [ 2.60241747e-01],\n",
              "        [-5.98664544e-01],\n",
              "        [ 4.02222995e-01],\n",
              "        [ 6.48013961e-02],\n",
              "        [-1.78033726e+00],\n",
              "        [ 2.58609783e-01],\n",
              "        [ 6.25102596e-01],\n",
              "        [-1.83128595e+00],\n",
              "        [-1.17435854e+00],\n",
              "        [-6.05971626e-01],\n",
              "        [-1.22548018e-01],\n",
              "        [-5.23399369e-01],\n",
              "        [-4.17555984e-01],\n",
              "        [-3.68608095e-01],\n",
              "        [-3.19487759e-01],\n",
              "        [-4.03217888e-01],\n",
              "        [-3.84006001e-01],\n",
              "        [-1.27226367e+00],\n",
              "        [ 7.38498564e-01],\n",
              "        [-2.02431405e+00],\n",
              "        [-2.33871834e+00],\n",
              "        [ 9.12423584e-02],\n",
              "        [ 5.89637649e-01],\n",
              "        [-1.34612628e-01],\n",
              "        [-9.99339935e-01],\n",
              "        [-7.10569802e-01],\n",
              "        [ 4.16059897e-01],\n",
              "        [-2.28319651e-01],\n",
              "        [-2.39676698e+00],\n",
              "        [-9.05853858e-01],\n",
              "        [-2.62324439e-01],\n",
              "        [ 4.47898203e-01],\n",
              "        [ 1.07681914e+00],\n",
              "        [ 4.48226812e-01],\n",
              "        [ 1.33187649e+00],\n",
              "        [-4.41980580e-01],\n",
              "        [-1.09145752e+00],\n",
              "        [-2.58680147e-01],\n",
              "        [ 9.15148001e-01],\n",
              "        [-9.15638547e-01]]),\n",
              " 'by': array([[ 2.34607863],\n",
              "        [ 1.67760411],\n",
              "        [-0.76849825],\n",
              "        [ 0.61529906],\n",
              "        [ 0.07545106],\n",
              "        [ 0.00284583],\n",
              "        [-1.35664935],\n",
              "        [-0.50708633],\n",
              "        [-0.18294567],\n",
              "        [ 0.89020095],\n",
              "        [-1.34638663],\n",
              "        [-0.45156189],\n",
              "        [-0.46339783],\n",
              "        [-0.05164372],\n",
              "        [ 0.49886447],\n",
              "        [ 0.78581001],\n",
              "        [ 0.1129011 ],\n",
              "        [-1.3096773 ],\n",
              "        [ 0.08010804],\n",
              "        [ 0.89656439],\n",
              "        [ 0.98319923],\n",
              "        [ 2.10635904],\n",
              "        [-0.79714886],\n",
              "        [-1.20197803],\n",
              "        [-1.1669153 ],\n",
              "        [-0.2703659 ],\n",
              "        [-1.19703086]])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    }
  ]
}